{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e261b6e-282f-4799-b9f2-5bd2e8e4133e",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "011de8c7-e0b1-487f-a16f-065320bde74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: “إذا أردت تنظيف الدرج فعليك أن تبدأ من الأعلى”.. مثال سواحلي بات شعارا مكتوبا على الهيئات الحكومية في تنزانيا؛ تلك الدولة الأفريقية صاحبة التجربة الرائدة في مكافحة الفساد.\n",
      "Text 2: منعت سلطات غينيا كوناكري الصحافة والمحامين والدبلوماسيين من حضور محاكمة أليو باه، مما أثار جدلا واسعا في البلاد، وسط اتهامات بتضييق الحريات وإسكات الأصوات المعارضة للنظام العسكري الحاكم.\n",
      "Text 3: فورتونا ماينينغ الكندية تقرر مغادرة بوركينا فاسو بعد بيع منجم ياراموكا بمبلغ 130 مليون دولار نقداً لشركة موريشيوسية، مبررة ذلك بتعقيدات مناخ الأعمال في البلاد.\n",
      "Text 4: وصل الرئيس فور غناسينغبي إلى كينشاسا كوسيط للاتحاد الأفريقي لحل النزاع بين الكونغو الديمقراطية ورواندا بعد فشل جهود وسيطين سابقين في تحقيق تقدم ملموس.\n",
      "Text 5: تمثل قصة النيجيري أوبافيمي مارتينز نموذجا حيا للإرادة، وتتجاوز كونها حكاية لاعب كرة قدم، لتصبح مثالا حيا على تقلبات الحياة. \n",
      "Text 6: صندوق النقد الدولي يفرج عن 129 مليون دولار لدعم مالي في مواجهة أزمة ميزان المدفوعات بعد الأضرار الناجمة عن الفيضانات الأخيرة، ويسمح لغامبيا بالوصول إلى سحب جزء من القروض التي تمت الموافقة عليها 2024.\n",
      "Text 7: المحكمة الفدرالية الأميركية تدين الجندي الغامبي السابق مايكل سانغ كوريا بتهمة التعذيب خلال حكم يحيى جامع، في خطوة قانونية تاريخية لملاحقة الجرائم العابرة للحدود.\n",
      "Text 8: اختطاف مواطنة سويسرية في أغاديز يعيد تسليط الضوء على هشاشة الأوضاع الأمنية شمال النيجر وتعاظم نشاط الجماعات المسلحة.\n",
      "Text 9: الصيغة الأميركية الجديدة تُحدد التعرفات الجمركية الأميركية على واردات أي دولة بما يعادل العجز التجاري الأميركي مع تلك الدولة مقسوما على وارداتها منها أو 10%، أيهما أعلى، مما أحدث زلزالا هزّ الأسواق.\n",
      "Text 10: أشاد وزير الخارجية الرواندي أوليفييه جان باتريك ندوهونغيره -اليوم الثلاثاء- بدور قطر في جهود الوساطة لحل النزاع مع الكونغو، مؤكداً أن بلاده تدعم بقوة مساعي الدوحة في هذا الشأن.\n",
      "Text 11: اعتقلت السلطات الكينية شابين بلجيكيين، وأحالتهما إلى القضاء بتهمة “قرصنة الحياة البرية” لحيازتهما أنابيب تحتوي على نحو خمسة آلاف نملة.\n",
      "Text 12: ملفات الطاقة وتأمين الحدود تتصدر أجندة زيارة وزير خارجية نيجيريا إلى النيجر، وسط مساع لتخفيف التوترات واستعادة الحركة التجارية.\n",
      "Text 13: السلطات المالية تغلق مكاتب “باريك غولد” في العاصمة باماكو بسبب النزاع حول الضرائب، وتمنع الموظفين من الوصول إلى أماكن عملهم وسط تهديدات بوضع منجم لولو تحت الوصاية.\n",
      "Text 14: هددت السنغال بمعاملة موريتانيا بالمثل إذا استمرت في ترحيل مواطنيها من دون احترام حقوقهم المتعلقة بالإقامة والعمل.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.aljazeera.net/africa/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Exemple de récupération des paragraphes\n",
    "articles = soup.find_all('p')\n",
    "texts = [p.text for p in articles if len(p.text.strip()) > 50]\n",
    "\n",
    "# Affichage\n",
    "for i, text in enumerate(texts[:20]):\n",
    "    print(f\"Text {i+1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0097515-9138-46d1-9eab-2d6b48ea4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Text': [texts[0], texts[1]],  # mets ceux que tu veux\n",
    "    'Score': [7.5, 6.0]  # attribue un score selon la pertinence\n",
    "}\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21e4b7c7-9561-47e4-a7fe-67807ce2ff1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“إذا أردت تنظيف الدرج فعليك أن تبدأ من الأعلى”...</td>\n",
       "      <td>أردت تنظيف الدرج فعليك تبدأ الأعلى مثال سواحلي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>منعت سلطات غينيا كوناكري الصحافة والمحامين وال...</td>\n",
       "      <td>منعت سلطات غينيا كوناكري الصحافة والمحامين وال...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  “إذا أردت تنظيف الدرج فعليك أن تبدأ من الأعلى”...   \n",
       "1  منعت سلطات غينيا كوناكري الصحافة والمحامين وال...   \n",
       "\n",
       "                                             Cleaned  \n",
       "0  أردت تنظيف الدرج فعليك تبدأ الأعلى مثال سواحلي...  \n",
       "1  منعت سلطات غينيا كوناكري الصحافة والمحامين وال...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Minimal Arabic stopwords list (expand as needed)\n",
    "arabic_stopwords = set([\n",
    "    \"في\", \"من\", \"على\", \"إلى\", \"عن\", \"ما\", \"لا\", \"لم\", \"لن\", \"إن\", \"أن\", \"كان\", \"قد\", \"هذه\",\n",
    "    \"هذا\", \"ذلك\", \"تلك\", \"هناك\", \"هنا\", \"أو\", \"و\", \"ثم\", \"بعد\", \"قبل\", \"كل\", \"أي\", \"أين\", \"كيف\",\n",
    "    \"لماذا\", \"هل\", \"بل\", \"حتى\", \"بين\", \"مثل\", \"مع\", \"إذا\", \"لكن\", \"لقد\", \"أكثر\", \"أقل\", \"بعض\", \"أحد\"\n",
    "])\n",
    "\n",
    "# Preprocessing function (simple tokenizer and stopword remover)\n",
    "def preprocess_arabic(text):\n",
    "    # Remove punctuation and digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove digits\n",
    "    text = text.strip()\n",
    "\n",
    "    # Tokenize by whitespace\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    cleaned_tokens = [word for word in tokens if word not in arabic_stopwords]\n",
    "\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# Example usage with a DataFrame column (assuming `df['Text']` exists)\n",
    "df['Cleaned'] = df['Text'].apply(preprocess_arabic)\n",
    "\n",
    "# Preview\n",
    "df[['Text', 'Cleaned']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5484ff54-675b-4fd0-b7d3-d4bb72b2703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample 1 ---\n",
      "Original Text :\n",
      "“إذا أردت تنظيف الدرج فعليك أن تبدأ من الأعلى”.. مثال سواحلي بات شعارا مكتوبا على الهيئات الحكومية في تنزانيا؛ تلك الدولة الأفريقية صاحبة التجربة الرائدة في مكافحة الفساد.\n",
      "\n",
      "Cleaned Text  :\n",
      "أردت تنظيف الدرج فعليك تبدأ الأعلى مثال سواحلي بات شعارا مكتوبا الهيئات الحكومية تنزانيا الدولة الأفريقية صاحبة التجربة الرائدة مكافحة الفساد\n",
      "\n",
      "Score         : 7.5\n",
      "----------------------------------------\n",
      "--- Sample 2 ---\n",
      "Original Text :\n",
      "منعت سلطات غينيا كوناكري الصحافة والمحامين والدبلوماسيين من حضور محاكمة أليو باه، مما أثار جدلا واسعا في البلاد، وسط اتهامات بتضييق الحريات وإسكات الأصوات المعارضة للنظام العسكري الحاكم.\n",
      "\n",
      "Cleaned Text  :\n",
      "منعت سلطات غينيا كوناكري الصحافة والمحامين والدبلوماسيين حضور محاكمة أليو باه مما أثار جدلا واسعا البلاد وسط اتهامات بتضييق الحريات وإسكات الأصوات المعارضة للنظام العسكري الحاكم\n",
      "\n",
      "Score         : 6.0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Loop through and print all rows\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"--- Sample {i+1} ---\")\n",
    "    print(\"Original Text :\")\n",
    "    print(row['Text'])\n",
    "    print(\"\\nCleaned Text  :\")\n",
    "    print(row['Cleaned'])\n",
    "    print(f\"\\nScore         : {row['Score']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab88dd2d-0872-4069-9630-702e48d0bf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN — MAE: 3.004, MSE: 10.154\n",
      "BiRNN — MAE: 1.026, MSE: 2.050\n",
      "GRUNet — MAE: 3.265, MSE: 12.655\n",
      "LSTMNet — MAE: 3.620, MSE: 14.870\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Your preprocessed dataset (from your lab3 notebook)\n",
    "texts = [\n",
    "    \"اندلعت اشتباكات عنيفة العاصمة إعلان نتائج الانتخابات\",\n",
    "    \"أعلنت الحكومة خطط جديدة لتعزيز الاقتصاد الوطني\",\n",
    "    \"لقي ثلاثة أشخاص مصرعهم حادث مروري مروع\",\n",
    "    \"تظاهر آلاف المواطنين شوارع المدينة للمطالبة بالإصلاح\",\n",
    "    \"سجلت وزارة الصحة ارتفاعا كبيرا حالات الإصابة بالفيروس\",\n",
    "    \"أطلقت الشركة التقنية منتجاً جديداً متوقع أن يغير السوق\",\n",
    "    \"شهدت البورصة ارتفاعا ملحوظا قيمة الأسهم\",\n",
    "    \"أعلن وزير التعليم تغييرات كبيرة النظام الدراسي\",\n",
    "    \"فاز المنتخب الوطني مباراة مثيرة ضد نظيره الإفريقي\",\n",
    "    \"تم توقيع اتفاقية تعاون البلدين لتعزيز العلاقات الثنائية\",\n",
    "    \"ارتفعت أسعار النفط عالمياً بسبب التوترات السياسية\",\n",
    "    \"نظمت الجمعية حملة تبرعات لمساعدة الأسر المحتاجة\",\n",
    "    \"تم اعتقال عدة أشخاص بتهم تتعلق بالفساد المالي\",\n",
    "    \"أعلنت المؤسسة منح دراسية جديدة للطلاب المتفوقين\",\n",
    "    \"أثارت التصريحات الأخيرة للزعيم جدلا واسعا الأوساط السياسية\",\n",
    "    \"أطلقت وكالة الفضاء قمرا صناعيا جديدا لمراقبة الطقس\",\n",
    "    \"تعرضت المدينة لعاصفة قوية خلفت أضرارا جسيمة\",\n",
    "    \"أعلنت نتائج الانتخابات المحلية بفوز الحزب الحاكم\",\n",
    "    \"افتتح المعرض الدولي للتكنولوجيا بحضور شركات عالمية\",\n",
    "    \"أطلقت السلطات حملة تطعيم وطنية ضد الأمراض المعدية\"\n",
    "]\n",
    "scores = [7.5, 8, 6.5, 9, 5, 8, 6, 7, 6.5, 9, 5.5, 7.5, 4, 8, 6, 9.5, 6.5, 7, 8.5, 6]\n",
    "\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X, padding='post')\n",
    "y = np.array(scores)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_tensor = torch.LongTensor(X_train)\n",
    "X_test_tensor = torch.LongTensor(X_test)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 64\n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "epochs = 10\n",
    "batch_size = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model Definitions\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.gru(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "# Train & Evaluate Function\n",
    "def train_and_evaluate(model_class, name):\n",
    "    model = model_class().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        permutation = torch.randperm(X_train_tensor.size(0))\n",
    "        for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x = X_train_tensor[indices].to(device)\n",
    "            batch_y = y_train_tensor[indices].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_test_tensor.to(device)).cpu().squeeze().numpy()\n",
    "        mae = np.mean(np.abs(preds - y_test))\n",
    "        mse = np.mean((preds - y_test) ** 2)\n",
    "\n",
    "    print(f\"{name} — MAE: {mae:.3f}, MSE: {mse:.3f}\")\n",
    "    return name, mae, mse\n",
    "\n",
    "# Run all models\n",
    "models = [SimpleRNN, BiRNN, GRUNet, LSTMNet]\n",
    "results = [train_and_evaluate(model, model.__name__) for model in models]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689294db-2545-4f0a-b760-486de93a8ea5",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "399200ed-355f-413a-be07-c5e01bcefb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /home/samirtaous/torchenv/lib/python3.12/site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/samirtaous/torchenv/lib/python3.12/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/samirtaous/torchenv/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/samirtaous/torchenv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/samirtaous/torchenv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/samirtaous/torchenv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/samirtaous/torchenv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/samirtaous/torchenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/samirtaous/torchenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/samirtaous/torchenv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/samirtaous/torchenv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/samirtaous/torchenv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/samirtaous/torchenv/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.30.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95e05a9-3801-4284-9a6a-c18df4d57327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samirtaous/torchenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-17 19:10:36.747639: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-17 19:10:36.759902: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744913436.771847   22362 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744913436.775250   22362 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744913436.784107   22362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744913436.784121   22362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744913436.784122   22362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744913436.784123   22362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-17 19:10:36.787116: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " الذكاء الاصطناعي في التعليم . . \" التعليم \" تحذر من استخدام تقنية \" الواقع المعزز \" في المدارس المصرية - فيديوشاهد بالفيديو . . \" التعليم \" تحذر من استخدام تقنية الواقع المعزز في المدارس المصريةأكدت وزارة التربية والتعليم والتعليم الفنى ، على أن التكنولوجيا ستغير من طريقة التدريس فى المدارس المصرية ، حيث ستقوم الوزارة بتطبيق نظام جديد من خلال \" التعليم \" وهو \" التعليم الإلكترونى \" الذى سيتم تطبيقه فى المدارس المصرية ، فى إطار الجهود التى تبذلها الوزارة مع وزارة التربية والتعليم والتعليم الفنى ، لرفع كفاءة المدارس وزيادة\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load Arabic GPT2 (or use 'gpt2' for English)\n",
    "model_name = \"aubmindlab/aragpt2-base\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"الذكاء الاصطناعي في التعليم\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    inputs,\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7806f000-43c1-4bf6-ad94-ac17590d36cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " التعليم في المستقبل سيكون على رأس أولوياتنا . . . . في ظل الأوضاع السياسية الراهنة التي تمر بها البلاد الآن . . . . سيكون مستقبل التعليم الفني على رأس أولوياتنا . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "BLEU-1: 0.0515\n",
      "BLEU-2: 0.0401\n",
      "BLEU-3: 0.0324\n",
      "BLEU-4: 0.0245\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import torch\n",
    "\n",
    "# Load Arabic GPT-2\n",
    "model_name = \"aubmindlab/aragpt2-base\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prompt\n",
    "prompt = \"التعليم في المستقبل سيكون\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(\n",
    "    inputs,\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and show output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\\n\", generated_text)\n",
    "\n",
    "# Reference(s)\n",
    "references = [\n",
    "    \"التعليم في المستقبل سيكون معتمداً على التكنولوجيا بشكل كبير\",\n",
    "    \"سيعتمد التعليم مستقبلاً على الذكاء الاصطناعي والتعلم الذاتي\",\n",
    "    \"المدارس ستستخدم تقنيات حديثة لتحسين التعلم\"\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "generated_tokens = generated_text.split()\n",
    "reference_tokens = [ref.split() for ref in references]\n",
    "\n",
    "# BLEU with smoothing\n",
    "smoother = SmoothingFunction().method4\n",
    "\n",
    "# BLEU scores\n",
    "for n in range(1, 5):\n",
    "    weights = tuple((1.0 / n if i < n else 0.0) for i in range(4))\n",
    "    score = sentence_bleu(reference_tokens, generated_tokens, weights=weights, smoothing_function=smoother)\n",
    "    print(f\"BLEU-{n}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
