{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34f68f7-3c26-4ed0-b30a-05a4cbc90635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import struct  # For reading binary data\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56eedd44-6ca5-4975-8a73-f8fd2f472d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Define paths to your ubyte files\n",
    "TRAIN_IMAGES_PATH = 'train-images.idx3-ubyte'\n",
    "TRAIN_LABELS_PATH = 'train-labels.idx1-ubyte' \n",
    "TEST_IMAGES_PATH = 't10k-images.idx3-ubyte' \n",
    "TEST_LABELS_PATH = 't10k-labels.idx1-ubyte' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd216601-19d0-4c0e-afb5-1f549e0ec65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Data Loading Function\n",
    "\n",
    "def read_idx(filename):\n",
    "    \"\"\"Reads an IDX file and returns a NumPy array.\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        if magic == 2051: # images\n",
    "            rows, cols = struct.unpack(\">II\", f.read(8))\n",
    "            data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "            data = data.reshape((size, rows, cols))\n",
    "        elif magic == 2049:  # labels\n",
    "            data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid magic number: {}\".format(magic))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ba4745-5b99-4c4d-95d5-56d468b03df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: CNN Model Definition\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 1 input channel (grayscale), 32 output channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 2x2 max pooling\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes (digits 0-9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the feature map\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2b794b-9231-401a-a05d-f5ba0c8c8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Device Configuration\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "134b966b-4c80-4685-9040-2ec8cf993c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Dataset for CNN\n",
    "\n",
    "class MNISTUByteDataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path, transform=None):\n",
    "        self.images = read_idx(images_path)\n",
    "        self.labels = read_idx(labels_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = np.expand_dims(image, axis=0) # Add channel dimension (1 for grayscale)\n",
    "        image = image.astype(np.float32) / 255.0 # Normalize\n",
    "\n",
    "        image = torch.from_numpy(image)  # Convert to Tensor\n",
    "\n",
    "        #No need to normalize if it is already between 0 and 1\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)  # Apply transformations (if any)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94f9376-9a69-4fbf-a664-46675d7aca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Dataset for Faster R-CNN\n",
    "\n",
    "class MNISTObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path):\n",
    "        self.images = read_idx(images_path)\n",
    "        self.labels = read_idx(labels_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert image to float32 and normalize\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image)\n",
    "        image = image.unsqueeze(0)  # Add channel dimension (C, H, W)\n",
    "\n",
    "        # Create bounding box (very simple: entire image as the bounding box)\n",
    "        # In a real object detection scenario, you'd have to do a better job with this.\n",
    "        # You'd ideally want a box that surrounds the digit.\n",
    "        #bounding_box = [0, 0, image.shape[2], image.shape[1]]  # xmin, ymin, xmax, ymax\n",
    "        #Better box:\n",
    "        bounding_box = [5, 5, image.shape[2]-5, image.shape[1]-5] #Give a box close to the actual image\n",
    "\n",
    "        boxes = torch.tensor([bounding_box], dtype=torch.float32)  # Wrap the bounding box in a tensor\n",
    "        labels = torch.tensor([label], dtype=torch.int64)  # Also wrap the labels in a tensor\n",
    "\n",
    "        # Create the target dictionary (required by Faster R-CNN)\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a83b6d-f834-4ec8-88b3-f586ceb087f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8: Create Dataloaders\n",
    "\n",
    "# Create CNN Datasets\n",
    "train_dataset_cnn = MNISTUByteDataset(TRAIN_IMAGES_PATH, TRAIN_LABELS_PATH)\n",
    "test_dataset_cnn = MNISTUByteDataset(TEST_IMAGES_PATH, TEST_LABELS_PATH)\n",
    "\n",
    "# Create CNN Data Loaders\n",
    "batch_size_cnn = 64  # Adjust as needed\n",
    "train_loader_cnn = DataLoader(train_dataset_cnn, batch_size=batch_size_cnn, shuffle=True)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=batch_size_cnn, shuffle=False)\n",
    "\n",
    "#Faster-RCNN:\n",
    "# Create Faster R-CNN Datasets\n",
    "train_dataset_frcnn = MNISTObjectDetectionDataset(TRAIN_IMAGES_PATH, TRAIN_LABELS_PATH)\n",
    "test_dataset_frcnn = MNISTObjectDetectionDataset(TEST_IMAGES_PATH, TEST_LABELS_PATH)\n",
    "\n",
    "# Create Faster R-CNN Data Loaders\n",
    "batch_size_frcnn = 1  # Adjust as needed (lower batch size due to memory constraints)\n",
    "train_loader_frcnn = DataLoader(train_dataset_frcnn, batch_size=batch_size_frcnn, shuffle=True)\n",
    "test_loader_frcnn = DataLoader(test_dataset_frcnn, batch_size=batch_size_frcnn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "280a45da-d0fd-425d-be27-767d802f5983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samirtaous/torchenv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Architecture:\n",
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Faster R-CNN Model Architecture:\n",
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      "  )\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0-3): 4 x Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=11, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=44, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Step 9: Instantiate Models\n",
    "\n",
    "# Instantiate CNN model\n",
    "cnn_model = CNN()\n",
    "cnn_model.to(device)\n",
    "\n",
    "# Instantiate Faster R-CNN model\n",
    "num_classes = 11  # 10 digits + background\n",
    "frcnn_model = fasterrcnn_resnet50_fpn(weights=True)\n",
    "in_features = frcnn_model.roi_heads.box_predictor.cls_score.in_features\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "frcnn_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "frcnn_model.to(device)\n",
    "\n",
    "print(\"CNN Model Architecture:\")\n",
    "print(cnn_model)\n",
    "\n",
    "print(\"\\nFaster R-CNN Model Architecture:\")\n",
    "print(frcnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be8e2d93-2922-4136-b189-96b3ec92a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 10: Training Function for CNN\n",
    "\n",
    "def train_cnn(model, train_loader, criterion, optimizer, device, num_epochs=5):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_accs = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] CNN loss: {running_loss / 200:.3f}')\n",
    "                train_losses.append(running_loss / 200)\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Evaluate on the test set after each epoch\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "            for data in test_loader_cnn:\n",
    "                images, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        val_accs.append(accuracy)\n",
    "        print(f'Epoch {epoch + 1}, CNN Test Accuracy: {accuracy:.2f}%')\n",
    "        model.train()  # Set the model to training mode again\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    print('Finished CNN Training')\n",
    "    return train_losses, val_accs, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dad1fdd-0cac-4789-b798-27c96c0e0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 11: Training Function for faster CNN (R-CNN)\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "def train_frcnn(model, train_loader, optimizer, device, num_epochs, max_batches=100):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    total_time = 0  # Track total training time\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}]:\")\n",
    "\n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            if batch_idx >= max_batches:\n",
    "                break  \n",
    "\n",
    "            images = [img.to(device, dtype=torch.float16, non_blocking=True) for img in images]\n",
    "\n",
    "            new_targets = []\n",
    "            for i in range(len(images)):\n",
    "                new_targets.append({\n",
    "                    \"boxes\": targets[\"boxes\"][i].view(-1, 4).to(device, non_blocking=True),\n",
    "                    \"labels\": targets[\"labels\"][i].view(-1).to(device, non_blocking=True)\n",
    "                })\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss_dict = model(images, new_targets)\n",
    "                loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 50 == 0 or batch_idx == max_batches - 1:\n",
    "                print(f\"  Batch [{batch_idx + 1}/{max_batches}] - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        total_time += epoch_time\n",
    "        avg_loss = epoch_loss / max_batches\n",
    "        print(f\"Epoch {epoch} completed in {epoch_time:.2f}s - Avg Loss: {avg_loss:.4f}\\n\")\n",
    "\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "    return losses, total_time  # Now returning both losses and training time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a685eede-7507-4406-93b9-75b473d51210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 12: Evaluation Function for CNN\n",
    "\n",
    "def evaluate_cnn(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predicted = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
    "    confusion_mat = confusion_matrix(all_labels, all_predicted)\n",
    "\n",
    "    return accuracy, f1, confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f793887-0faf-4528-9194-9b981c508173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 13: Evaluation Function for faster CNN (R-CNN)\n",
    "\n",
    "# Ensure the model is in float16 for mixed precision\n",
    "frcnn_model = frcnn_model.half()  # Cast the model to float16\n",
    "\n",
    "def evaluate_frcnn(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_predicted = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, targets) in enumerate(test_loader):\n",
    "            # Break after processing 100 batches\n",
    "            if idx >= 100:\n",
    "                break\n",
    "\n",
    "            # Print progress for each batch\n",
    "            print(f\"Processing batch {idx + 1}/100...\")\n",
    "\n",
    "            images = [img.to(device, dtype=torch.float16, non_blocking=True) for img in images]  # float16 inputs\n",
    "\n",
    "            # Convert targets to the same format used in the training function\n",
    "            new_targets = []\n",
    "            for i in range(len(images)):\n",
    "                new_targets.append({\n",
    "                    \"boxes\": targets[\"boxes\"][i].view(-1, 4).to(device, non_blocking=True),\n",
    "                    \"labels\": targets[\"labels\"][i].view(-1).to(device, non_blocking=True)\n",
    "                })\n",
    "\n",
    "            outputs = model(images, new_targets)\n",
    "\n",
    "            # Log the shape of outputs\n",
    "            print(f\"Batch {idx + 1}: Number of outputs: {len(outputs)}\")\n",
    "            for i, output in enumerate(outputs):\n",
    "                print(f\"  Output {i}: Boxes: {output['boxes'].shape}, Labels: {output['labels'].shape}, Scores: {output['scores'].shape}\")\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                boxes = output['boxes']\n",
    "                labels = output['labels']\n",
    "                scores = output['scores']\n",
    "\n",
    "                # Filter predictions based on a confidence threshold\n",
    "                confidence_threshold = 0.5  # Adjust as needed\n",
    "                filtered_indices = scores > confidence_threshold\n",
    "\n",
    "                predicted_labels = labels[filtered_indices].cpu().numpy()\n",
    "                true_labels = new_targets[i]['labels'].cpu().numpy()  # Accessing labels from new_targets\n",
    "\n",
    "                all_predicted.extend(predicted_labels)\n",
    "                all_labels.extend(true_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(np.array(all_predicted) == np.array(all_labels)) * 100 if all_labels else 0\n",
    "    f1 = f1_score(all_labels, all_predicted, average='weighted', zero_division=0) if all_labels else 0\n",
    "    confusion_mat = confusion_matrix(all_labels, all_predicted) if all_labels else np.array([[0]])  # Handle empty case\n",
    "\n",
    "    return accuracy, f1, confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "924b8661-b699-4be8-a9f9-ce070d36fbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] CNN loss: 0.515\n",
      "[1,   400] CNN loss: 0.134\n",
      "[1,   600] CNN loss: 0.102\n",
      "[1,   800] CNN loss: 0.082\n",
      "Epoch 1, CNN Test Accuracy: 97.98%\n",
      "[2,   200] CNN loss: 0.057\n",
      "[2,   400] CNN loss: 0.054\n",
      "[2,   600] CNN loss: 0.052\n",
      "[2,   800] CNN loss: 0.047\n",
      "Epoch 2, CNN Test Accuracy: 98.67%\n",
      "[3,   200] CNN loss: 0.036\n",
      "[3,   400] CNN loss: 0.036\n",
      "[3,   600] CNN loss: 0.036\n",
      "[3,   800] CNN loss: 0.037\n",
      "Epoch 3, CNN Test Accuracy: 98.92%\n",
      "[4,   200] CNN loss: 0.023\n",
      "[4,   400] CNN loss: 0.027\n",
      "[4,   600] CNN loss: 0.027\n",
      "[4,   800] CNN loss: 0.026\n",
      "Epoch 4, CNN Test Accuracy: 99.01%\n",
      "[5,   200] CNN loss: 0.017\n",
      "[5,   400] CNN loss: 0.022\n",
      "[5,   600] CNN loss: 0.019\n",
      "[5,   800] CNN loss: 0.021\n",
      "Epoch 5, CNN Test Accuracy: 98.83%\n",
      "Finished CNN Training\n",
      "\n",
      "CNN Results:\n",
      "  Accuracy: 98.83%\n",
      "  F1-score: 0.988\n",
      "  Training Time: 25.68 seconds\n"
     ]
    }
   ],
   "source": [
    "#Step 14: Training & Evaluation loops\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# CNN Training\n",
    "cnn_criterion = nn.CrossEntropyLoss()\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "cnn_train_losses, cnn_val_accs, cnn_training_time = train_cnn(cnn_model, train_loader_cnn, cnn_criterion, cnn_optimizer, device, num_epochs=5)\n",
    "\n",
    "# CNN Evaluation\n",
    "cnn_accuracy, cnn_f1_score, cnn_confusion_mat = evaluate_cnn(cnn_model, test_loader_cnn, device)\n",
    "print(\"\\nCNN Results:\")\n",
    "print(f\"  Accuracy: {cnn_accuracy:.2f}%\")\n",
    "print(f\"  F1-score: {cnn_f1_score:.3f}\")\n",
    "print(f\"  Training Time: {cnn_training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77da580f-8732-46fd-a11a-c3bbc458b17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18814/3524787208.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_18814/3524787208.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:\n",
      "  Batch [50/100] - Loss: 0.1660\n",
      "  Batch [100/100] - Loss: 0.2619\n",
      "Epoch 1 completed in 179.40s - Avg Loss: 0.2104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Faster R-CNN Training\n",
    "frcnn_params = [p for p in frcnn_model.parameters() if p.requires_grad]\n",
    "frcnn_optimizer = optim.Adam(frcnn_params, lr=0.0005) # Reduce learning rate\n",
    "\n",
    "frcnn_train_losses, frcnn_training_time = train_frcnn(frcnn_model, train_loader_frcnn, frcnn_optimizer, device, num_epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e423a113-e42c-41e6-b33e-7f0ffcbbd92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/100...\n",
      "Batch 1: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 2/100...\n",
      "Batch 2: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 3/100...\n",
      "Batch 3: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 4/100...\n",
      "Batch 4: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 5/100...\n",
      "Batch 5: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 6/100...\n",
      "Batch 6: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 7/100...\n",
      "Batch 7: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 8/100...\n",
      "Batch 8: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 9/100...\n",
      "Batch 9: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 10/100...\n",
      "Batch 10: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 11/100...\n",
      "Batch 11: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 12/100...\n",
      "Batch 12: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 13/100...\n",
      "Batch 13: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 14/100...\n",
      "Batch 14: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 15/100...\n",
      "Batch 15: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 16/100...\n",
      "Batch 16: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 17/100...\n",
      "Batch 17: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 18/100...\n",
      "Batch 18: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 19/100...\n",
      "Batch 19: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 20/100...\n",
      "Batch 20: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 21/100...\n",
      "Batch 21: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 22/100...\n",
      "Batch 22: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 23/100...\n",
      "Batch 23: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 24/100...\n",
      "Batch 24: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 25/100...\n",
      "Batch 25: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 26/100...\n",
      "Batch 26: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 27/100...\n",
      "Batch 27: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 28/100...\n",
      "Batch 28: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 29/100...\n",
      "Batch 29: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 30/100...\n",
      "Batch 30: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 31/100...\n",
      "Batch 31: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 32/100...\n",
      "Batch 32: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 33/100...\n",
      "Batch 33: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 34/100...\n",
      "Batch 34: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 35/100...\n",
      "Batch 35: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 36/100...\n",
      "Batch 36: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 37/100...\n",
      "Batch 37: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 38/100...\n",
      "Batch 38: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 39/100...\n",
      "Batch 39: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 40/100...\n",
      "Batch 40: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 41/100...\n",
      "Batch 41: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 42/100...\n",
      "Batch 42: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 43/100...\n",
      "Batch 43: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 44/100...\n",
      "Batch 44: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 45/100...\n",
      "Batch 45: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 46/100...\n",
      "Batch 46: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 47/100...\n",
      "Batch 47: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 48/100...\n",
      "Batch 48: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 49/100...\n",
      "Batch 49: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 50/100...\n",
      "Batch 50: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 51/100...\n",
      "Batch 51: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 52/100...\n",
      "Batch 52: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 53/100...\n",
      "Batch 53: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 54/100...\n",
      "Batch 54: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 55/100...\n",
      "Batch 55: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 56/100...\n",
      "Batch 56: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 57/100...\n",
      "Batch 57: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 58/100...\n",
      "Batch 58: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 59/100...\n",
      "Batch 59: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 60/100...\n",
      "Batch 60: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 61/100...\n",
      "Batch 61: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 62/100...\n",
      "Batch 62: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 63/100...\n",
      "Batch 63: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 64/100...\n",
      "Batch 64: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 65/100...\n",
      "Batch 65: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 66/100...\n",
      "Batch 66: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 67/100...\n",
      "Batch 67: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 68/100...\n",
      "Batch 68: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 69/100...\n",
      "Batch 69: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 70/100...\n",
      "Batch 70: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 71/100...\n",
      "Batch 71: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 72/100...\n",
      "Batch 72: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 73/100...\n",
      "Batch 73: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 74/100...\n",
      "Batch 74: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 75/100...\n",
      "Batch 75: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 76/100...\n",
      "Batch 76: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 77/100...\n",
      "Batch 77: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 78/100...\n",
      "Batch 78: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 79/100...\n",
      "Batch 79: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 80/100...\n",
      "Batch 80: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 81/100...\n",
      "Batch 81: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 82/100...\n",
      "Batch 82: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 83/100...\n",
      "Batch 83: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 84/100...\n",
      "Batch 84: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 85/100...\n",
      "Batch 85: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 86/100...\n",
      "Batch 86: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 87/100...\n",
      "Batch 87: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 88/100...\n",
      "Batch 88: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 89/100...\n",
      "Batch 89: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 90/100...\n",
      "Batch 90: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 91/100...\n",
      "Batch 91: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 92/100...\n",
      "Batch 92: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 93/100...\n",
      "Batch 93: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 94/100...\n",
      "Batch 94: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 95/100...\n",
      "Batch 95: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 96/100...\n",
      "Batch 96: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 97/100...\n",
      "Batch 97: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 98/100...\n",
      "Batch 98: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 99/100...\n",
      "Batch 99: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n",
      "Processing batch 100/100...\n",
      "Batch 100: Number of outputs: 1\n",
      "  Output 0: Boxes: torch.Size([0, 4]), Labels: torch.Size([0]), Scores: torch.Size([0])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (0,) (100,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Faster R-CNN Evaluation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m frcnn_accuracy, frcnn_f1_score, frcnn_confusion_mat = \u001b[43mevaluate_frcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrcnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader_frcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFaster R-CNN Results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfrcnn_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mevaluate_frcnn\u001b[39m\u001b[34m(model, test_loader, device)\u001b[39m\n\u001b[32m     50\u001b[39m             all_labels.extend(true_labels)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m accuracy = np.mean(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_predicted\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m) * \u001b[32m100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m all_labels \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     54\u001b[39m f1 = f1_score(all_labels, all_predicted, average=\u001b[33m'\u001b[39m\u001b[33mweighted\u001b[39m\u001b[33m'\u001b[39m, zero_division=\u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m all_labels \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     55\u001b[39m confusion_mat = confusion_matrix(all_labels, all_predicted) \u001b[38;5;28;01mif\u001b[39;00m all_labels \u001b[38;5;28;01melse\u001b[39;00m np.array([[\u001b[32m0\u001b[39m]])  \u001b[38;5;66;03m# Handle empty case\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (0,) (100,) "
     ]
    }
   ],
   "source": [
    "# Faster R-CNN Evaluation\n",
    "frcnn_accuracy, frcnn_f1_score, frcnn_confusion_mat = evaluate_frcnn(frcnn_model, test_loader_frcnn, device)\n",
    "print(\"\\nFaster R-CNN Results:\")\n",
    "print(f\"  Accuracy: {frcnn_accuracy:.2f}%\")\n",
    "print(f\"  F1-score: {frcnn_f1_score:.3f}\")\n",
    "print(f\"  Training Time: {frcnn_training_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
